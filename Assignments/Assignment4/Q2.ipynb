{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "martial-barcelona",
   "metadata": {},
   "source": [
    "# Assignment 4 - Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-scanner",
   "metadata": {},
   "source": [
    "## Modeling Frog Escape Problem as Finite MDP (Code from last assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-lexington",
   "metadata": {},
   "source": [
    "For the *frog-escape problem*, we have the state space $S = \\{0, 1, ..., n\\}$, the terminal states $T = {0, n}$, and the action space $A = {A, B}$ (croaking). \n",
    "\n",
    "We have the following transition probabilities $\\mathcal{P}(s,a,s') \\text{ for all } i \\in N$:\n",
    "\n",
    "$$\\mathcal{P}(i,A,i-1) = \\frac{i}{n}$$\n",
    "$$\\mathcal{P}(i,A,i+1) = \\frac{n-1}{i}$$\n",
    "$$\\mathcal{P}(i,B, s') = \\frac{1}{n} \\text{ for all }s' \\in \\{0,...,i-1,i+1,...n\\}$$\n",
    "\n",
    "All other transition probabilities are 0. Finally, we set the reward function $\\mathcal{R_T}(s,a,s')$ to be:\n",
    "\n",
    "$$\\mathcal{R_T}(i,A,n) = 1 \\text{ for all } i \\in N $$\n",
    "$$\\mathcal{R_T}(i,B,n) = 1 \\text{ for all } i \\in N $$\n",
    "\n",
    "Rewards for landing all other states are 0. Then, the only way to get any reward is by escaping (landing on n), and the Optimal Value Function will represent the probability of escaping the bond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "judicial-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "from scipy.stats import poisson\n",
    "import itertools\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expired-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PadState:\n",
    "    pad_num: int\n",
    "        \n",
    "PadTransMapping = StateActionMapping[PadState, int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chubby-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrogEscMDP(FiniteMarkovDecisionProcess[PadState, int]):\n",
    "\n",
    "    def __init__(self, n: int):\n",
    "        self.n: int = n     \n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> PadTransMapping:\n",
    "        #state -> action -> state-reward -> p \n",
    "        d: Dict[PadState, Dict[str, Categorical[Tuple[PadState, float]]]] = {}\n",
    "        #define rewards\n",
    "        rewards: Dict[int, float] = {s: 0 for s in range(n)}\n",
    "        rewards[n] = 1\n",
    "            \n",
    "        for state in range(1, self.n): #non-terminal states\n",
    "            #action -> state-reward -> p\n",
    "            d1: Dict[str, Categorical[Tuple[PadState, float]]] = {}\n",
    "            \n",
    "            #croaks A\n",
    "            #state-reward -> p (to be turned into Categorical)\n",
    "            sr_probs_dict_A: Dict[Tuple[PadState, float], float] = \\\n",
    "                {(PadState(state-1), rewards[state-1]): state/n, \\\n",
    "                (PadState(state+1), rewards[state+1]): (n-state)/n}\n",
    "            d1['A'] = Categorical(sr_probs_dict_A)\n",
    "            \n",
    "            #croaks B\n",
    "            sr_probs_dict_B: Dict[Tuple[PadState, float], float] = \\\n",
    "                {(PadState(next_state), rewards[next_state]): 1/n for next_state in range(n+1) if n != state}          \n",
    "            d1['B'] = Categorical(sr_probs_dict_B)\n",
    "\n",
    "            d[PadState(state)] = d1\n",
    "            \n",
    "        #terminal states\n",
    "        d[PadState(0)] = None\n",
    "        d[PadState(n)] = None\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "first-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "n = 6\n",
    "frog_mdp: FiniteMarkovDecisionProcess[PadState, int] =\\\n",
    "    FrogEscMDP(n=n)\n",
    "\n",
    "# print(\"MDP Transition Map\")\n",
    "# print(\"------------------\")\n",
    "# print(frog_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "falling-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.dynamic_programming import policy_iteration_result\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "interior-truck",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rl.distribution.Categorical"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(frog_mdp.mapping[PadState(1)]['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "declared-caution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(PadState(pad_num=0), 0): 0.16666666666666666, (PadState(pad_num=2), 0): 0.8333333333333334}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frog_mdp.mapping[PadState(1)]['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "related-chambers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PadState(pad_num=0), 0)\n",
      "0.16666666666666666\n",
      "(PadState(pad_num=2), 0)\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "for x,p in frog_mdp.mapping[PadState(1)]['A']:\n",
    "    print(x)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "focused-qualification",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {(PadState(pad_num=0), 0): 0.16666666666666666, (PadState(pad_num=2), 0): 0.8333333333333334},\n",
       " 'B': {(PadState(pad_num=0), 0): 0.14285714285714288, (PadState(pad_num=1), 0): 0.14285714285714288, (PadState(pad_num=2), 0): 0.14285714285714288, (PadState(pad_num=3), 0): 0.14285714285714288, (PadState(pad_num=4), 0): 0.14285714285714288, (PadState(pad_num=5), 0): 0.14285714285714288, (PadState(pad_num=6), 1): 0.14285714285714288}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frog_mdp.mapping[PadState(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "split-memphis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PadState(pad_num=1): {'A': {(PadState(pad_num=0), 0): 0.16666666666666666, (PadState(pad_num=2), 0): 0.8333333333333334},\n",
       "  'B': {(PadState(pad_num=0), 0): 0.14285714285714288, (PadState(pad_num=1), 0): 0.14285714285714288, (PadState(pad_num=2), 0): 0.14285714285714288, (PadState(pad_num=3), 0): 0.14285714285714288, (PadState(pad_num=4), 0): 0.14285714285714288, (PadState(pad_num=5), 0): 0.14285714285714288, (PadState(pad_num=6), 1): 0.14285714285714288}},\n",
       " PadState(pad_num=2): {'A': {(PadState(pad_num=1), 0): 0.3333333333333333, (PadState(pad_num=3), 0): 0.6666666666666666},\n",
       "  'B': {(PadState(pad_num=0), 0): 0.14285714285714288, (PadState(pad_num=1), 0): 0.14285714285714288, (PadState(pad_num=2), 0): 0.14285714285714288, (PadState(pad_num=3), 0): 0.14285714285714288, (PadState(pad_num=4), 0): 0.14285714285714288, (PadState(pad_num=5), 0): 0.14285714285714288, (PadState(pad_num=6), 1): 0.14285714285714288}},\n",
       " PadState(pad_num=3): {'A': {(PadState(pad_num=2), 0): 0.5, (PadState(pad_num=4), 0): 0.5},\n",
       "  'B': {(PadState(pad_num=0), 0): 0.14285714285714288, (PadState(pad_num=1), 0): 0.14285714285714288, (PadState(pad_num=2), 0): 0.14285714285714288, (PadState(pad_num=3), 0): 0.14285714285714288, (PadState(pad_num=4), 0): 0.14285714285714288, (PadState(pad_num=5), 0): 0.14285714285714288, (PadState(pad_num=6), 1): 0.14285714285714288}},\n",
       " PadState(pad_num=4): {'A': {(PadState(pad_num=3), 0): 0.6666666666666666, (PadState(pad_num=5), 0): 0.3333333333333333},\n",
       "  'B': {(PadState(pad_num=0), 0): 0.14285714285714288, (PadState(pad_num=1), 0): 0.14285714285714288, (PadState(pad_num=2), 0): 0.14285714285714288, (PadState(pad_num=3), 0): 0.14285714285714288, (PadState(pad_num=4), 0): 0.14285714285714288, (PadState(pad_num=5), 0): 0.14285714285714288, (PadState(pad_num=6), 1): 0.14285714285714288}},\n",
       " PadState(pad_num=5): {'A': {(PadState(pad_num=4), 0): 0.8333333333333334, (PadState(pad_num=6), 1): 0.16666666666666666},\n",
       "  'B': {(PadState(pad_num=0), 0): 0.14285714285714288, (PadState(pad_num=1), 0): 0.14285714285714288, (PadState(pad_num=2), 0): 0.14285714285714288, (PadState(pad_num=3), 0): 0.14285714285714288, (PadState(pad_num=4), 0): 0.14285714285714288, (PadState(pad_num=5), 0): 0.14285714285714288, (PadState(pad_num=6), 1): 0.14285714285714288}},\n",
       " PadState(pad_num=0): None,\n",
       " PadState(pad_num=6): None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frog_mdp.mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "turned-coupon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(frog_mdp.mapping[PadState(2)]['A']).expectation(lambda s_r: s_r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-ferry",
   "metadata": {},
   "source": [
    "## Using functions policy_iteration and value_iteration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "grand-serum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Policy Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{PadState(pad_num=4): 0.7444417962236649,\n",
      " PadState(pad_num=5): 0.7870264522069902,\n",
      " PadState(pad_num=1): 0.6594154577345575,\n",
      " PadState(pad_num=2): 0.7019054789306673,\n",
      " PadState(pad_num=3): 0.7231639388837591}\n",
      "For State PadState(pad_num=1):\n",
      "  Do Action B with Probability 1.000\n",
      "For State PadState(pad_num=2):\n",
      "  Do Action A with Probability 1.000\n",
      "For State PadState(pad_num=3):\n",
      "  Do Action A with Probability 1.000\n",
      "For State PadState(pad_num=4):\n",
      "  Do Action A with Probability 1.000\n",
      "For State PadState(pad_num=5):\n",
      "  Do Action A with Probability 1.000\n",
      "\n",
      "It took 53.347110748291016 ms for policy iteration\n",
      "\n",
      "MDP Value Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{PadState(pad_num=4): 0.7444351834022599,\n",
      " PadState(pad_num=5): 0.7870207101314249,\n",
      " PadState(pad_num=1): 0.6594110592638028,\n",
      " PadState(pad_num=2): 0.7018993329449162,\n",
      " PadState(pad_num=3): 0.7231572915541348}\n",
      "For State PadState(pad_num=1):\n",
      "  Do Action B with Probability 1.000\n",
      "For State PadState(pad_num=2):\n",
      "  Do Action A with Probability 1.000\n",
      "For State PadState(pad_num=3):\n",
      "  Do Action A with Probability 1.000\n",
      "For State PadState(pad_num=4):\n",
      "  Do Action A with Probability 1.000\n",
      "For State PadState(pad_num=5):\n",
      "  Do Action A with Probability 1.000\n",
      "\n",
      "It took 24.434328079223633 ms for value iteration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"MDP Policy Iteration Optimal Value Function and Optimal Policy\")\n",
    "print(\"--------------\")\n",
    "t1 = time.time()\n",
    "opt_vf_pi, opt_policy_pi = policy_iteration_result(\n",
    "    frog_mdp,\n",
    "    gamma=1.0\n",
    ")\n",
    "t2 = time.time()\n",
    "pprint(opt_vf_pi)\n",
    "print(opt_policy_pi)\n",
    "print(f\"It took {(t2-t1)*1000} ms for policy iteration\")\n",
    "print()\n",
    "\n",
    "print(\"MDP Value Iteration Optimal Value Function and Optimal Policy\")\n",
    "print(\"--------------\")\n",
    "t3 = time.time()\n",
    "opt_vf_vi, opt_policy_vi = value_iteration_result(frog_mdp, gamma=1.0)\n",
    "t4 = time.time()\n",
    "pprint(opt_vf_vi)\n",
    "print(opt_policy_vi)\n",
    "print(f\"It took {(t4-t3)*1000} ms for value iteration\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-palace",
   "metadata": {},
   "source": [
    "Value iteration is much faster as we had expected since it performs partial policy evaluation per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CME241: Markov Processes and Markov Reward Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we model the game of *Snakes and Ladders* as an MP, we have the state space S = {0,1,...,99,100} where 0 is the starting state and 100 is the terminal state. If we ignore snakes and ladders, the transition probability from state $s$ to to $s+1,s+2...,s+6$ is $\\frac{1}{6}$. We can use mapping $M: S \\rightarrow S $ in our model to account for snakes and ladders. For example, if $s+1$ is mapped to $s+10$ in the ladder mapping, the transition probability from $s$ to $s+1$ will be replaced with zero and from $s$ to $s+10$ will be $\\frac{1}{6}$. Another edge case when the next state is > 100, the player remains where he is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Mapping, Dict\n",
    "from rl.distribution import Choose, Constant\n",
    "from rl.markov_process import Transition, FiniteMarkovProcess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakes = {\n",
    "    16: 6,\n",
    "    47: 26,\n",
    "    49: 11,\n",
    "    56: 53,\n",
    "    62: 19,\n",
    "    64: 60,\n",
    "    87: 24,\n",
    "    93: 73,\n",
    "    95: 75,\n",
    "    98: 78\n",
    "}\n",
    "\n",
    "ladders = {\n",
    "    1: 38, \n",
    "    4: 14, \n",
    "    9: 31,\n",
    "    28: 84,\n",
    "    21: 42,\n",
    "    36: 44,\n",
    "    51: 67,\n",
    "    71: 91,\n",
    "    80: 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a transition map\n",
    "sl_transition_map: Transition[int]= {}\n",
    "states = [s for s in range(100) if s not in snakes and s not in ladders] #non-terminal states\n",
    "for state in states:\n",
    "    destinations = []\n",
    "    for roll_value in range(1,7):\n",
    "        next_state = state + roll_value\n",
    "        #replace next states if snake or ladder\n",
    "        if next_state in snakes:\n",
    "            next_state = snakes[next_state]\n",
    "        if next_state in ladders:\n",
    "            next_state = ladders[next_state]\n",
    "        #piece does not move if next state is above 100\n",
    "        if next_state > 100:\n",
    "            next_state = state\n",
    "        destinations.append(next_state)\n",
    "    #select an element of the given list uniformly at random\n",
    "    sl_transition_map[state] = Choose(destinations)\n",
    "sl_transition_map[100] = None #winning condition (terminal state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model the game as an instance of FiniteMarkovProcess\n",
    "game = FiniteMarkovProcess(sl_transition_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the method traces to create sampling traces\n",
    "time_steps = []\n",
    "num_simulations = 0\n",
    "for trace in game.traces(Constant(0)):\n",
    "    time_steps.append(len(list(trace)))\n",
    "    num_simulations += 1\n",
    "    if num_simulations > 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a graph of the probability distribution of time steps to finish the game.\n",
    "plt.hist(x=time_steps, bins='auto');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected number of steps to finish the game is 40.08109189081092.\n"
     ]
    }
   ],
   "source": [
    "print(\"The expected number of steps to finish the game is {}.\".format(np.mean(time_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we make slight changes to Q2 solutions to solve the frog puzzle. Given a frog and a distance n, we have the state space S = {0,1,...,n} where 0 is the starting state and n is the terminal state. At state $s$, it can jump to $s+1, ..., n$ with equal probability. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a transition map\n",
    "def get_frog_transition_map(n: int):\n",
    "    frog_transition_map: Transition[int]= {}\n",
    "    states = [s for s in range(n)] #non-terminal states\n",
    "    for state in states:\n",
    "        destinations = []\n",
    "        for num_steps in range(1,n-state+1):\n",
    "            next_state = state + num_steps\n",
    "            destinations.append(next_state)\n",
    "        frog_transition_map[state] = Choose(destinations)\n",
    "    frog_transition_map[n] = None #terminating condition when frog reaches the end\n",
    "    return frog_transition_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model the game as an instance of FiniteMarkovProcess\n",
    "dist = 10\n",
    "frog_game = FiniteMarkovProcess(get_frog_transition_map(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For distance of 10, the expected number of jumps is 2.9212078792120786.\n"
     ]
    }
   ],
   "source": [
    "#use the method traces to create sampling traces\n",
    "num_jumps = []\n",
    "num_simulations = 0\n",
    "for trace in frog_game.traces(Constant(0)):\n",
    "    #the number of steps in each simulation will be the number of states visited - 1 (not counting start state)\n",
    "    num_jumps.append(len(list(trace))-1)\n",
    "    num_simulations += 1\n",
    "    if num_simulations > 10000:\n",
    "        break\n",
    "exp_num_jumps = np.mean(num_jumps)\n",
    "print(\"For distance of {}, the expected number of jumps is {}.\".format(dist, exp_num_jumps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can model Q2 as an MRP by adding a reward of 1 for every transition, and using discount rate $\\gamma = 1$. Then, we can use the value function of state $s$ will give the expected number of dice rolls to finish the game from that state. We can apply the value function to the starting state $s = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.markov_process import FiniteMarkovRewardProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_reward_transition_map = {}\n",
    "for state, next_dist in sl_transition_map.items():\n",
    "    if next_dist:\n",
    "        destinations = [(next_state, 1) for next_state in next_dist.options]\n",
    "        sl_reward_transition_map[state] = Choose(destinations)\n",
    "    else:\n",
    "        sl_reward_transition_map[state] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_mrp = FiniteMarkovRewardProcess(sl_reward_transition_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected number of steps to finish the game is 36.96052442581669.\n"
     ]
    }
   ],
   "source": [
    "exp_num_steps = sl_mrp.get_value_function_vec(1)[0]\n",
    "print(\"The expected number of steps to finish the game is {}.\".format(exp_num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
